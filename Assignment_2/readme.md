# Back Propogation in Neural Networks

## Screenshot of the Network training in excel

### Model and Gradient Calculation
![image](https://user-images.githubusercontent.com/24980224/118105606-378b3d00-b3fa-11eb-8ab0-ff6fd5f76db3.png)

### Training of network in Excel
![image](https://user-images.githubusercontent.com/24980224/118106281-0f500e00-b3fb-11eb-9daf-135fb278a0b1.png)

## Drop in error at different Learning Rates
<img src="https://user-images.githubusercontent.com/24980224/118105293-d4010f80-b3f9-11eb-9323-c1a70f2d4664.png" width="400" height="300"/>

Here it can be noticed that the rate at which the error has decreased is faster in this example with the increase in the learning rate. Generally, it is preferred to keep the learning rate < 1   

## Below is the explanation of the model

In the below network we have input layer (i1,i2) , a hidden layer (a_h1,a_h2) and an output layer (a_o1,a_o2). The mean squared errors from the output layer is summed and represented as E_total

![image](https://user-images.githubusercontent.com/24980224/118081121-642e5d00-b3d8-11eb-8ad2-f422454b94f8.png)

Here i1 and i2 are inputs to layer 1 and w1, w2, w3, w4 are corresponding weights for the layers and below is the calculations done in layer 1

![image](https://user-images.githubusercontent.com/24980224/118082954-b6bd4880-b3db-11eb-8fad-86c72adc06dc.png)

Here a_h1 and a_h2 are inputs to layer 2 and w5, w6, w7, w8 are corresponding weights for the layers and below is the calculations done in layer 2 or output

![image](https://user-images.githubusercontent.com/24980224/118083113-f6843000-b3db-11eb-9e68-fb8337debe9b.png)

Here is the final error generated by the network 

![image](https://user-images.githubusercontent.com/24980224/118083137-03a11f00-b3dc-11eb-9f08-a99d34f03f8c.png)

## Below is the calculation of gradients w.r.t to weights in the network

### Gradient Calculation
Here gradients are calculated using the chain rule, and the calculated gradients are used to update the weights of the layers. In each iteration, we will use a fraction of gradient value (using learning rate) to update the weights and re-calculate the error.
											
∂E_total /∂w5 = ∂(E1+E2)/∂w5 = ∂E1/∂w5 = (∂E1/∂a_o1) * (∂a_o1/∂o1) *(∂o1/∂w5)											

∂E1/∂a_o1 = ∂(1/2 *(t1-a_o1)2)/∂a_o1 = (t1-a_o1) *(-1) = a_o1-t1											
∂a_o1/∂o1 = ∂(σ(o1))/∂o1 = ∂(1/1+e-o1)/∂o1 = (1+e-o1)-2 *(-1)*(-1)*(e-o1) = (1/(1+e-o1))*((e-o1)/(1+e-o1)) = a_o1*(1-a_o1)											
∂o1/∂w5 = ∂(w5*a_h1+w6*a_h2) /∂w5 = a_h1

∂E_total /∂w5 = (a_o1 - t1) * (a_o1)* (1-a_o1) * a_h1											
∂E_total /∂w6 = (a_o1 - t1) * (a_o1)* (1-a_o1) * a_h2											
∂E_total /∂w7 = (a_o2 - t2) * (a_o2)* (1-a_o2) * a_h1											
∂E_total /∂w8 = (a_o2 - t2) * (a_o2)* (1-a_o2) * a_h2											
											
∂E_total /∂a_h1 = ∂(E1+E2)/∂a_h1 = ∂E1/∂a_h1 + ∂E2/∂a_h1											

∂E1/∂a_h1 = ∂E1/∂a_o1* ∂a_o1/∂o1 *∂o1/da_h1 = (a_o1-t1) * a_o1 *(1-a_o1) *w5 											
∂E2/∂a_h1 = ∂E2/∂a_o2* ∂a_o2/∂o2 *∂o2/da_h1 = (a_o2-t2) * a_o2 *(1-a_o2) *w7 											
∂E_total /∂a_h1 = (a_o1-t1) * a_o1 *(1-a_o1) *w5  + (a_o2-t2) * a_o2 *(1-a_o2) *w7 											
∂E_total /∂a_h2 = (a_o1-t1) * a_o1 *(1-a_o1) *w6  + (a_o2-t2) * a_o2 *(1-a_o2) *w8											
											
∂E_total /∂w1 =  ∂E_total/∂a_h1  *∂a_h1/∂h1 *∂h1/∂w1											

∂E_total /∂w1 =  ( ∂E_total/∂a_h1) * (a_h1)*(1-a_h1)*i1											
∂E_total /∂w2 =  ( ∂E_total/∂a_h1) * (a_h1)*(1-a_h1)*i2											
∂E_total /∂w3 =  ( ∂E_total/∂a_h2) * (a_h2)*(1-a_h2)*i1											
∂E_total /∂w4 =  ( ∂E_total/∂a_h2) * (a_h2)*(1-a_h2)*i2											

![image](https://user-images.githubusercontent.com/24980224/118128813-079f6200-b419-11eb-9c28-a4e1cd868043.png)

